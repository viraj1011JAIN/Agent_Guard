# üéØ AgentGuard Complete Implementation Checklist

**Complete End-to-End Deployment Guide**

*From Zero to Production-Ready AI Agent Security Platform*

---

## üìã How to Use This Checklist

### Structure
- **Phases**: Major milestones in the project (0-6)
- **Tasks**: Specific actionable items under each phase
- **Subtasks**: Granular steps within tasks
- **Success Criteria**: How to verify each phase is complete
- **Expected Outputs**: What you should see/have after completion

### Conventions
- ‚òê Unchecked task
- ‚úÖ Completed task
- ‚ö†Ô∏è Critical task (blocking next phase)
- üîç Verification step
- üìä Expected output
- ‚ö° Performance validation
- üîí Security validation

### Progress Tracking
After each phase, verify ALL success criteria before proceeding.

---

## Phase 0: Foundation & Environment Setup

**Goal**: Establish development environment and project structure

**Prerequisites**: None (starting from scratch)

**Estimated Complexity**: Basic

---

### Task 0.1: Windows Environment Preparation (if using Windows)

#### Subtasks:

‚òê **0.1.1** Check Windows version
- Open Settings ‚Üí System ‚Üí About
- Verify Windows 10 version 2004+ or Windows 11
- Note down build number

‚òê **0.1.2** Enable virtualization in BIOS
- Restart computer
- Enter BIOS/UEFI (usually F2, F10, or Del key)
- Navigate to CPU settings
- Enable Intel VT-x or AMD-V
- Save and exit

‚òê **0.1.3** Enable Windows features
- Open Control Panel
- Navigate to Programs and Features
- Click "Turn Windows features on or off"
- Enable: Virtual Machine Platform
- Enable: Windows Subsystem for Linux
- Restart computer

‚òê **0.1.4** Install WSL2
- Open PowerShell as Administrator
- Run WSL installation command
- Set WSL2 as default version
- Install Ubuntu distribution
- Restart computer

‚òê **0.1.5** Configure WSL2 resources
- Create .wslconfig file in user directory
- Set memory limit (16GB recommended)
- Set processor count (8 cores recommended)
- Set swap size (8GB recommended)
- Enable localhost forwarding
- Restart WSL

#### Success Criteria:
üîç **Verification Steps:**
- Open Ubuntu terminal successfully
- Check WSL version shows "2"
- Verify WSL can access Windows filesystem
- Check allocated resources match configuration

üìä **Expected Outputs:**
- Ubuntu terminal opens without errors
- WSL version command returns "WSL version 2"
- Can navigate to /mnt/c directory
- Memory and CPU limits are respected

---

### Task 0.2: Install Core Development Tools in WSL2

#### Subtasks:

‚òê **0.2.1** Update system packages
- Open WSL2 Ubuntu terminal
- Update package lists
- Upgrade all installed packages
- Install build essentials
- Install common utilities (curl, wget, git)

‚òê **0.2.2** Install Docker Desktop for Windows
- Download Docker Desktop installer
- Run installer with default settings
- Start Docker Desktop application
- Wait for Docker engine to start
- Enable WSL2 backend in Docker settings
- Enable Ubuntu integration in Docker settings

‚òê **0.2.3** Verify Docker in WSL2
- Open WSL2 terminal
- Check Docker version
- Check Docker Compose version
- Run test container
- Verify container runs successfully
- Remove test container

‚òê **0.2.4** Install Go programming language
- Download Go binary for Linux
- Extract to /usr/local
- Add Go to PATH in bashrc
- Reload shell configuration
- Verify Go installation
- Check Go version
- Test Go workspace

‚òê **0.2.5** Install Python
- Check if Python 3.11 is available in apt
- Install Python 3.11
- Install python3-pip
- Install python3-venv
- Verify Python installation
- Check Python version
- Test pip installation

‚òê **0.2.6** Install Node.js and npm
- Add NodeSource repository
- Install Node.js LTS version
- Verify Node.js installation
- Verify npm installation
- Update npm to latest version
- Test npm package installation

‚òê **0.2.7** Install Kubernetes tools
- Download kubectl binary
- Install kubectl to /usr/local/bin
- Make kubectl executable
- Verify kubectl installation
- Download Helm installer script
- Run Helm installation
- Verify Helm installation

‚òê **0.2.8** Install k3d (Kubernetes in Docker)
- Download k3d installer script
- Run k3d installation
- Verify k3d installation
- Check k3d version
- Test k3d cluster creation capability

‚òê **0.2.9** Install additional utilities
- Install make
- Install jq (JSON processor)
- Install yq (YAML processor)
- Install tree (directory visualization)
- Verify all utilities are accessible

#### Success Criteria:
üîç **Verification Steps:**
- All version commands return successfully
- Docker runs containers without sudo
- Go can compile simple programs
- Python can install packages
- Node can run JavaScript files
- kubectl can connect to clusters
- Helm can list repositories
- k3d can list clusters

üìä **Expected Outputs:**
- Docker version shows client and server info
- Go version shows 1.21 or higher
- Python version shows 3.11 or higher
- Node version shows v20.x.x
- kubectl version shows client version
- Helm version shows v3.x.x
- k3d version shows installed version
- All utilities respond to help commands

---

### Task 0.3: Project Repository Setup

#### Subtasks:

‚òê **0.3.1** Create project directory structure
- Create main project directory
- Navigate to project directory
- Initialize Git repository
- Create .gitignore file
- Add common ignore patterns

‚òê **0.3.2** Create component directories
- Create agent-runtime directory
- Create ebpf-collector directory
- Create security-brain directory
- Create tool-proxy directory
- Create red-team directory
- Create web-ui directory
- Create deploy directory
- Create docs directory
- Create benchmarks directory
- Create examples directory

‚òê **0.3.3** Create configuration directories
- Create config directory in project root
- Create subdirectory for OPA policies
- Create subdirectory for Kubernetes manifests
- Create subdirectory for Helm charts
- Create subdirectory for environment configs

‚òê **0.3.4** Create documentation structure
- Create README.md placeholder
- Create CONTRIBUTING.md placeholder
- Create LICENSE file (Apache 2.0)
- Create CODE_OF_CONDUCT.md placeholder
- Create docs/architecture directory
- Create docs/api directory
- Create docs/deployment directory

‚òê **0.3.5** Initialize component README files
- Create README.md in each component directory
- Add component description placeholders
- Add build instructions placeholders
- Add testing instructions placeholders

‚òê **0.3.6** Create Makefile for automation
- Create Makefile in project root
- Add phony targets declaration
- Add help target with descriptions
- Add clean target
- Add basic build targets

‚òê **0.3.7** Set up version control
- Configure Git user name
- Configure Git user email
- Create initial commit
- Create development branch
- Create feature branch naming convention
- Document Git workflow in CONTRIBUTING.md

#### Success Criteria:
üîç **Verification Steps:**
- Directory tree shows all required folders
- Git status shows clean working directory
- All README files are present
- Makefile help target displays correctly
- Git log shows initial commit
- Can create and switch branches

üìä **Expected Outputs:**
- Tree command shows organized directory structure
- Git status returns "nothing to commit"
- Each component has its own README
- Makefile displays available targets
- Git log shows commit history
- Branch listing shows main and development branches

---

### Task 0.4: Development Environment Configuration

#### Subtasks:

‚òê **0.4.1** Configure VSCode (or preferred IDE)
- Install Visual Studio Code
- Install WSL extension
- Connect to WSL workspace
- Install Go extension
- Install Python extension
- Install Docker extension
- Install Kubernetes extension
- Install YAML extension

‚òê **0.4.2** Set up editor workspace
- Open project directory in editor
- Configure editor settings for Go
- Configure editor settings for Python
- Configure editor settings for TypeScript
- Set up recommended extensions list
- Configure format on save
- Set up linting rules

‚òê **0.4.3** Configure terminal environment
- Set up shell aliases for common commands
- Configure shell prompt with Git info
- Set up command history search
- Configure tab completion
- Create project-specific shell functions

‚òê **0.4.4** Set up secrets management
- Create .env.example file
- Document required environment variables
- Add .env to .gitignore
- Create secrets directory structure
- Document secrets management approach

‚òê **0.4.5** Configure debugging setup
- Set up Go debugging configuration
- Set up Python debugging configuration
- Configure Docker debugging
- Create launch configurations for common scenarios
- Test debugging capabilities

#### Success Criteria:
üîç **Verification Steps:**
- Editor opens project without errors
- Extensions are installed and active
- Linting works for all languages
- Debugging breakpoints work
- Environment variables load correctly

üìä **Expected Outputs:**
- VSCode shows no extension errors
- Go files show syntax highlighting and errors
- Python files show linting suggestions
- Breakpoints stop execution correctly
- Shell aliases work as expected

---

### Task 0.5: Container Registry Setup

#### Subtasks:

‚òê **0.5.1** Choose container registry
- Decide between Docker Hub, GitHub Container Registry, or private registry
- Create account if needed
- Verify access credentials

‚òê **0.5.2** Configure local Docker authentication
- Log in to container registry
- Verify authentication success
- Test image push capability
- Test image pull capability

‚òê **0.5.3** Set up image naming convention
- Define image naming scheme
- Define tagging strategy (latest, version tags)
- Document naming convention
- Create environment variables for registry URL

‚òê **0.5.4** Create base images planning
- List all required base images
- Document base image dependencies
- Plan layer optimization strategy
- Document multi-stage build approach

#### Success Criteria:
üîç **Verification Steps:**
- Can authenticate to registry
- Can push test image
- Can pull test image
- Naming convention is documented

üìä **Expected Outputs:**
- Docker login shows "Login Succeeded"
- Test image pushes without errors
- Test image appears in registry
- Documentation clearly explains naming

---

### PHASE 0 COMPLETION CHECKLIST

**Before proceeding to Phase 1, verify ALL of the following:**

‚úÖ WSL2 is installed and running Ubuntu
‚úÖ Docker Desktop is running and integrated with WSL2
‚úÖ Can run Docker containers without sudo
‚úÖ Go version 1.21+ is installed and accessible
‚úÖ Python 3.11+ is installed and accessible
‚úÖ Node.js 20+ is installed and accessible
‚úÖ kubectl, Helm, and k3d are installed
‚úÖ Project directory structure is created
‚úÖ Git repository is initialized with initial commit
‚úÖ All component directories have README files
‚úÖ Makefile has basic targets
‚úÖ IDE is configured with necessary extensions
‚úÖ Can authenticate to container registry
‚úÖ Development environment is fully functional

**Expected State:**
- Complete development environment ready
- Project structure established
- Version control initialized
- Tools verified and working
- Ready to start building components

---

## Phase 1: Local Kubernetes Cluster & Infrastructure

**Goal**: Set up local Kubernetes cluster and deploy core infrastructure services

**Prerequisites**: Phase 0 completed successfully

**Estimated Complexity**: Intermediate

---

### Task 1.1: Create Local Kubernetes Cluster

#### Subtasks:

‚òê **1.1.1** Plan cluster configuration
- Decide on number of nodes (recommend 3: 1 control plane, 2 workers)
- Plan resource allocation per node
- Document cluster specifications
- Plan networking configuration

‚òê **1.1.2** Create k3d cluster
- Run k3d cluster creation command
- Specify cluster name
- Configure node count
- Set up port forwarding for ingress
- Mount required volumes (eBPF paths)
- Wait for cluster creation to complete

‚òê **1.1.3** Verify cluster creation
- Check cluster status
- List all nodes
- Verify all nodes are Ready
- Check system pods are running
- Verify networking is working

‚òê **1.1.4** Configure kubectl context
- List available contexts
- Set k3d cluster as current context
- Verify context is set correctly
- Test kubectl commands against cluster

‚òê **1.1.5** Install cluster utilities
- Deploy metrics-server (for resource monitoring)
- Verify metrics-server deployment
- Test metrics collection
- Deploy dashboard (optional)

‚òê **1.1.6** Configure cluster networking
- Verify CoreDNS is running
- Test DNS resolution in cluster
- Check pod-to-pod networking
- Test service discovery

#### Success Criteria:
üîç **Verification Steps:**
- Cluster info command returns cluster details
- All nodes show Ready status
- Can create and delete test pods
- Pods can communicate with each other
- Services are accessible from pods
- Metrics API responds to queries

üìä **Expected Outputs:**
- Cluster info shows Kubernetes control plane URL
- Node list shows all nodes in Ready state
- System pods are all Running
- Test pod deploys and runs successfully
- Can resolve service names from pods
- Metrics command returns CPU and memory usage

---

### Task 1.2: Create Kubernetes Namespaces

#### Subtasks:

‚òê **1.2.1** Create agentguard-system namespace
- Create namespace for AgentGuard infrastructure
- Add labels for organization
- Add annotations for documentation
- Verify namespace creation

‚òê **1.2.2** Create agentguard-agents namespace
- Create namespace for AI agents
- Add labels for organization
- Configure resource quotas
- Configure network policies preparation
- Verify namespace creation

‚òê **1.2.3** Create monitoring namespace
- Create namespace for observability stack
- Add labels for organization
- Verify namespace creation

‚òê **1.2.4** Configure namespace defaults
- Set default resource limits
- Set default resource requests
- Configure service accounts
- Set up RBAC rules

‚òê **1.2.5** Document namespace strategy
- Document purpose of each namespace
- Document resource allocation strategy
- Document security boundaries
- Add to project documentation

#### Success Criteria:
üîç **Verification Steps:**
- Can list all namespaces
- Each namespace has correct labels
- Can deploy resources to each namespace
- Resource quotas are in effect
- Default limits are applied to new pods

üìä **Expected Outputs:**
- Namespace list shows three namespaces
- Each namespace has description annotation
- Test deployment respects resource limits
- Pods inherit default service account

---

### Task 1.3: Deploy Apache Kafka (Event Streaming)

#### Subtasks:

‚òê **1.3.1** Add Helm repository
- Add Bitnami Helm repository
- Update Helm repository cache
- Search for Kafka chart
- Verify chart is available

‚òê **1.3.2** Prepare Kafka configuration
- Create values file for Kafka deployment
- Configure replica count (1 for dev, 3 for prod)
- Set persistence size
- Configure auto-create topics setting
- Configure resource requests and limits
- Set JVM memory settings

‚òê **1.3.3** Deploy Kafka using Helm
- Install Kafka chart to agentguard-system namespace
- Monitor deployment progress
- Wait for all pods to be ready
- Check pod logs for errors

‚òê **1.3.4** Verify Kafka deployment
- List Kafka pods
- Check pod status is Running
- Verify persistent volumes are bound
- Check Kafka logs show successful startup

‚òê **1.3.5** Test Kafka functionality
- Create test topic
- List all topics
- Produce test message to topic
- Consume test message from topic
- Delete test topic
- Verify basic operations work

‚òê **1.3.6** Create AgentGuard topics
- Create topic for raw events
- Create topic for decisions
- Create topic for alerts
- Set appropriate partitions for each
- Set retention policies
- Verify topics are created

‚òê **1.3.7** Document Kafka endpoints
- Note down Kafka bootstrap server address
- Document topic naming convention
- Document partition strategy
- Add connection details to documentation

#### Success Criteria:
üîç **Verification Steps:**
- Kafka pods are Running
- Can list topics successfully
- Can produce and consume messages
- Topics have correct partition counts
- Persistence works across pod restarts

üìä **Expected Outputs:**
- Kafka-0 pod shows Running status
- Topic list includes agentguard topics
- Test message is successfully consumed
- Topics show correct configurations
- Connection from within cluster works

---

### Task 1.4: Deploy TimescaleDB (Event Storage)

#### Subtasks:

‚òê **1.4.1** Add TimescaleDB Helm repository
- Add Timescale Helm repository
- Update Helm repository cache
- Search for TimescaleDB chart
- Verify chart is available

‚òê **1.4.2** Prepare TimescaleDB configuration
- Create values file for TimescaleDB
- Configure persistence size
- Set PostgreSQL parameters
- Configure max connections
- Set resource requests and limits
- Configure backup settings

‚òê **1.4.3** Deploy TimescaleDB using Helm
- Install TimescaleDB chart to agentguard-system namespace
- Monitor deployment progress
- Wait for StatefulSet to be ready
- Check pod logs for errors

‚òê **1.4.4** Verify TimescaleDB deployment
- List TimescaleDB pods
- Check pod status is Running
- Verify persistent volume is bound
- Check PostgreSQL is accepting connections

‚òê **1.4.5** Access TimescaleDB shell
- Execute shell command in TimescaleDB pod
- Connect to PostgreSQL using psql
- Verify TimescaleDB extension is loaded
- Check database version
- Exit database shell

‚òê **1.4.6** Create AgentGuard database
- Connect to PostgreSQL as postgres user
- Create agentguard database
- Verify database creation
- List all databases

‚òê **1.4.7** Initialize database schema
- Connect to agentguard database
- Create events table with appropriate columns
- Add timestamp, agent_id, event_type columns
- Add syscall, tool, path, network columns
- Add decision, risk_score columns
- Add metadata JSONB column
- Set primary key on timestamp and id

‚òê **1.4.8** Create hypertable for time-series
- Convert events table to hypertable
- Partition by timestamp
- Verify hypertable creation
- Check hypertable settings

‚òê **1.4.9** Add database indexes
- Create index on agent_id and timestamp
- Create index on syscall where not null
- Create index on decision where not null
- Create index on event_type
- Verify all indexes are created

‚òê **1.4.10** Configure compression policy
- Enable compression on events table
- Set compression segment by agent_id
- Add compression policy for data older than 7 days
- Verify compression policy is active

‚òê **1.4.11** Configure retention policy
- Add retention policy for data older than 90 days
- Verify retention policy is active
- Document retention settings

‚òê **1.4.12** Test database functionality
- Insert test event row
- Query test event back
- Update test event
- Delete test event
- Verify all CRUD operations work

‚òê **1.4.13** Document database connection
- Note down connection string
- Document database credentials location
- Document schema design
- Add connection details to documentation

#### Success Criteria:
üîç **Verification Steps:**
- TimescaleDB pod is Running
- Can connect to database
- AgentGuard database exists
- Events table is created as hypertable
- All indexes are present
- Compression and retention policies are active
- Can perform CRUD operations

üìä **Expected Outputs:**
- timescaledb-0 pod shows Running status
- Database list includes agentguard
- Table list shows events table
- Hypertable info shows events is hypertable
- Index list shows all created indexes
- Policy list shows compression and retention policies
- Test queries return expected results

---

### Task 1.5: Deploy Redis (Approval Queue)

#### Subtasks:

‚òê **1.5.1** Prepare Redis configuration
- Create values file for Redis deployment
- Configure auth settings (disable for dev)
- Set persistence size
- Configure resource requests and limits
- Set memory limits

‚òê **1.5.2** Deploy Redis using Helm
- Install Redis chart to agentguard-system namespace
- Monitor deployment progress
- Wait for pods to be ready
- Check pod logs for errors

‚òê **1.5.3** Verify Redis deployment
- List Redis pods
- Check pod status is Running
- Verify persistent volume is bound
- Check Redis is accepting connections

‚òê **1.5.4** Test Redis functionality
- Connect to Redis using redis-cli
- Run PING command
- Set test key-value pair
- Get test value back
- Delete test key
- Verify operations work

‚òê **1.5.5** Document Redis connection
- Note down Redis service address
- Document connection method
- Add connection details to documentation

#### Success Criteria:
üîç **Verification Steps:**
- Redis master pod is Running
- PING returns PONG
- Can set and get values
- Persistence works across pod restarts

üìä **Expected Outputs:**
- redis-master-0 pod shows Running status
- PING command returns "PONG"
- GET returns previously SET value
- Data survives pod restart

---

### Task 1.6: Deploy Temporal (Workflow Engine)

#### Subtasks:

‚òê **1.6.1** Add Temporal Helm repository
- Add Temporal Helm repository
- Update Helm repository cache
- Search for Temporal chart
- Verify chart is available

‚òê **1.6.2** Prepare Temporal configuration
- Create values file for Temporal deployment
- Configure server replica count
- Configure Cassandra cluster size
- Disable optional components (Prometheus, Grafana)
- Set resource requests and limits

‚òê **1.6.3** Deploy Temporal using Helm
- Install Temporal chart to agentguard-system namespace
- Monitor deployment progress (this takes several minutes)
- Wait for all pods to be ready
- Check pod logs for errors

‚òê **1.6.4** Verify Temporal deployment
- List Temporal pods
- Check all pods are Running
- Verify Cassandra is ready
- Check Temporal frontend is ready
- Check Temporal history service is ready

‚òê **1.6.5** Test Temporal functionality
- Access Temporal frontend pod
- Verify Temporal CLI is available
- List Temporal namespaces
- Verify default namespace exists

‚òê **1.6.6** Create AgentGuard namespace in Temporal
- Register agentguard namespace
- Verify namespace creation
- List namespaces to confirm

‚òê **1.6.7** Document Temporal connection
- Note down frontend service address
- Document namespace details
- Add connection details to documentation

#### Success Criteria:
üîç **Verification Steps:**
- All Temporal pods are Running
- Cassandra is accepting connections
- Frontend service is accessible
- Can list namespaces
- AgentGuard namespace exists

üìä **Expected Outputs:**
- Temporal frontend pod shows Running
- Temporal history pod shows Running
- Cassandra pods show Running
- Namespace list includes agentguard
- No errors in pod logs

---

### Task 1.7: Deploy Prometheus & Grafana (Monitoring - Optional)

#### Subtasks:

‚òê **1.7.1** Add Prometheus Helm repository
- Add prometheus-community Helm repository
- Update Helm repository cache
- Search for kube-prometheus-stack chart
- Verify chart is available

‚òê **1.7.2** Prepare monitoring stack configuration
- Create values file for monitoring deployment
- Configure Prometheus retention period
- Configure storage size
- Set resource requests and limits
- Configure Grafana admin password

‚òê **1.7.3** Deploy monitoring stack
- Install kube-prometheus-stack to monitoring namespace
- Monitor deployment progress
- Wait for all pods to be ready
- Check pod logs for errors

‚òê **1.7.4** Verify monitoring deployment
- List monitoring pods
- Check all pods are Running
- Verify Prometheus is collecting metrics
- Verify Grafana is accessible

‚òê **1.7.5** Access Grafana dashboard
- Set up port-forward to Grafana service
- Open browser to Grafana
- Log in with admin credentials
- Verify default dashboards are available

‚òê **1.7.6** Configure data sources
- Verify Prometheus data source is configured
- Test data source connection
- Check metrics are being collected

‚òê **1.7.7** Document monitoring access
- Note down Grafana URL
- Document admin credentials location
- Document port-forward command
- Add monitoring details to documentation

#### Success Criteria:
üîç **Verification Steps:**
- Prometheus pod is Running
- Grafana pod is Running
- Can access Grafana UI
- Data source shows "Working"
- Can see cluster metrics in dashboards

üìä **Expected Outputs:**
- All monitoring pods show Running status
- Grafana login page loads
- Default dashboards display data
- Prometheus query interface works
- Cluster metrics are visible

---

### Task 1.8: Configure Secrets Management

#### Subtasks:

‚òê **1.8.1** Create placeholder for LLM API keys
- Prepare structure for Anthropic API key secret
- Document secret naming convention
- Create example secret YAML (without actual keys)
- Add to .gitignore

‚òê **1.8.2** Create placeholder for Slack webhook
- Prepare structure for Slack webhook secret
- Document webhook configuration
- Create example secret YAML
- Add to .gitignore

‚òê **1.8.3** Create placeholder for registry credentials
- Prepare structure for container registry secret
- Document registry authentication
- Create example secret YAML
- Add to .gitignore

‚òê **1.8.4** Document secrets management
- Document how to create secrets
- Document where to store actual credentials
- Document rotation procedures
- Add security best practices

#### Success Criteria:
üîç **Verification Steps:**
- Example secret files are documented
- .gitignore includes secret files
- Documentation explains secret creation
- No actual credentials in repository

üìä **Expected Outputs:**
- Example YAML files are templated
- README explains secret setup
- Git status shows no secret files
- Documentation is clear and complete

---

### PHASE 1 COMPLETION CHECKLIST

**Before proceeding to Phase 2, verify ALL of the following:**

‚úÖ Local Kubernetes cluster is running
‚úÖ All nodes show Ready status
‚úÖ Three namespaces are created (system, agents, monitoring)
‚úÖ Kafka is deployed and accepting connections
‚úÖ Can create and list Kafka topics
‚úÖ TimescaleDB is deployed and initialized
‚úÖ AgentGuard database and schema exist
‚úÖ Hypertable, indexes, and policies are configured
‚úÖ Redis is deployed and responding to PING
‚úÖ Temporal is deployed with all services running
‚úÖ AgentGuard namespace exists in Temporal
‚úÖ (Optional) Prometheus and Grafana are accessible
‚úÖ Secret management structure is documented
‚úÖ Can connect to all infrastructure services

**Expected State:**
- Complete infrastructure platform ready
- All databases initialized
- Event streaming operational
- Workflow engine ready
- Ready to build AgentGuard components

**Test the Complete Stack:**
- Create test Kafka topic and send messages
- Insert and query test data in TimescaleDB
- Set and get values in Redis
- List Temporal namespaces
- View cluster metrics in Grafana (if installed)

---

## Phase 2: eBPF Observability Layer

**Goal**: Build kernel-level monitoring system using eBPF

**Prerequisites**: Phase 1 completed successfully

**Estimated Complexity**: Advanced

---

### Task 2.1: Set Up eBPF Development Environment

#### Subtasks:

‚òê **2.1.1** Install eBPF development tools
- Install clang compiler
- Install llvm tools
- Install libbpf development headers
- Install linux-headers for current kernel
- Verify kernel version supports eBPF
- Install bpftool utility

‚òê **2.1.2** Verify eBPF support
- Check kernel CONFIG_BPF is enabled
- Check kernel CONFIG_BPF_SYSCALL is enabled
- Verify /sys/kernel/debug/tracing exists
- Check bpf filesystem is mounted
- Test loading simple eBPF program

‚òê **2.1.3** Install Go eBPF library
- Navigate to ebpf-collector directory
- Initialize Go module
- Add cilium/ebpf dependency
- Download module dependencies
- Verify module is ready

‚òê **2.1.4** Create project structure
- Create bpf subdirectory for eBPF C programs
- Create userspace subdirectory for Go collector
- Create pkg subdirectory for packages
- Create cmd subdirectory for main application
- Create README with build instructions

‚òê **2.1.5** Set up build system
- Create Makefile for building eBPF programs
- Add target to compile C to eBPF bytecode
- Add target to generate Go bindings
- Add target to build Go binary
- Add clean target
- Test build process

#### Success Criteria:
üîç **Verification Steps:**
- All development tools are installed
- Kernel supports required eBPF features
- Go module initializes without errors
- Directory structure is organized
- Build system compiles simple program

üìä **Expected Outputs:**
- clang version shows installed
- bpftool can list loaded programs
- Go mod verify shows no errors
- Makefile successfully builds test program
- Generated binaries are created

---

### Task 2.2: Develop File Monitoring eBPF Program

#### Subtasks:

‚òê **2.2.1** Create eBPF program skeleton
- Create C file for file monitoring
- Include necessary kernel headers
- Define eBPF map structures
- Define event structure
- Add license declaration

‚òê **2.2.2** Define file event structure
- Add timestamp field
- Add process ID field
- Add user ID field
- Add filename field with size limit
- Add flags field
- Add mode field

‚òê **2.2.3** Create ring buffer map
- Define ring buffer map for events
- Set appropriate size
- Document map purpose

‚òê **2.2.4** Create monitored cgroups map
- Define hash map for cgroup filtering
- Set key type to cgroup ID
- Set value type to boolean
- Set max entries limit

‚òê **2.2.5** Implement openat tracepoint
- Attach to sys_enter_openat tracepoint
- Check if cgroup is monitored
- Extract event details from context
- Populate event structure
- Read filename from userspace
- Submit event to ring buffer

‚òê **2.2.6** Implement read tracepoint
- Attach to sys_enter_read tracepoint
- Filter for monitored cgroups
- Extract file descriptor
- Extract byte count
- Submit event to ring buffer

‚òê **2.2.7** Implement write tracepoint
- Attach to sys_enter_write tracepoint
- Filter for monitored cgroups
- Extract file descriptor
- Extract byte count
- Submit event to ring buffer

‚òê **2.2.8** Compile eBPF program
- Use Makefile to compile C to eBPF bytecode
- Generate BTF (BPF Type Format) information
- Verify compilation succeeds
- Check generated object file

‚òê **2.2.9** Test eBPF program loading
- Write minimal Go program to load eBPF
- Load compiled eBPF object
- Verify maps are created
- Verify programs are loaded
- Check for any errors

#### Success Criteria:
üîç **Verification Steps:**
- eBPF program compiles without errors
- Object file is generated successfully
- Can load eBPF program into kernel
- Maps are visible in bpftool
- Programs are attached to tracepoints

üìä **Expected Outputs:**
- Compilation produces .o file
- bpftool prog list shows loaded programs
- bpftool map list shows ring buffer and cgroup map
- No verification errors during load
- Programs show attached status

---

### Task 2.3: Develop Network Monitoring eBPF Program

#### Subtasks:

‚òê **2.3.1** Create network event structure
- Define structure for network events
- Add timestamp, PID, UID fields
- Add destination IP field
- Add destination port field
- Add bytes transferred field
- Add protocol field

‚òê **2.3.2** Implement socket tracepoint
- Attach to sys_enter_socket tracepoint
- Filter for monitored cgroups
- Extract socket family
- Extract socket type
- Submit event to ring buffer

‚òê **2.3.3** Implement connect tracepoint
- Attach to sys_enter_connect tracepoint
- Filter for monitored cgroups
- Extract socket file descriptor
- Extract destination address
- Parse IP and port
- Submit event to ring buffer

‚òê **2.3.4** Implement sendto tracepoint
- Attach to sys_enter_sendto tracepoint
- Filter for monitored cgroups
- Extract bytes being sent
- Extract destination if provided
- Submit event to ring buffer

‚òê **2.3.5** Implement recvfrom tracepoint
- Attach to sys_enter_recvfrom tracepoint
- Filter for monitored cgroups
- Extract bytes received
- Submit event to ring buffer

‚òê **2.3.6** Compile and test network program
- Add network program to Makefile
- Compile eBPF program
- Test loading into kernel
- Verify tracepoints attach successfully

#### Success Criteria:
üîç **Verification Steps:**
- Network eBPF program compiles
- Programs attach to network tracepoints
- Can capture network events
- IP addresses are correctly parsed
- Port numbers are extracted

üìä **Expected Outputs:**
- Network monitoring object file created
- bpftool shows network programs loaded
- Test network activity generates events
- Events contain correct IP and port info

---

### Task 2.4: Develop Process Monitoring eBPF Program

#### Subtasks:

‚òê **2.4.1** Create process event structure
- Define structure for process events
- Add timestamp, PID, PPID fields
- Add UID, GID fields
- Add command name field
- Add arguments field (limited size)
- Add return code field

‚òê **2.4.2** Implement execve tracepoint
- Attach to sys_enter_execve tracepoint
- Filter for monitored cgroups
- Extract program path
- Extract arguments
- Submit event to ring buffer

‚òê **2.4.3** Implement clone/fork tracepoint
- Attach to sched_process_fork tracepoint
- Filter for monitored cgroups
- Extract parent and child PIDs
- Submit event to ring buffer

‚òê **2.4.4** Implement exit tracepoint
- Attach to sched_process_exit tracepoint
- Filter for monitored cgroups
- Extract PID and exit code
- Submit event to ring buffer

‚òê **2.4.5** Compile and test process program
- Add process program to Makefile
- Compile eBPF program
- Test loading into kernel
- Verify tracepoints attach successfully

#### Success Criteria:
üîç **Verification Steps:**
- Process eBPF program compiles
- Programs attach to process tracepoints
- Can capture process creation
- Can capture process termination
- Command arguments are captured

üìä **Expected Outputs:**
- Process monitoring object file created
- bpftool shows process programs loaded
- Test process execution generates events
- Events contain command and arguments

---

### Task 2.5: Build Go Userspace Collector

#### Subtasks:

‚òê **2.5.1** Create collector main structure
- Create main.go in cmd/collector
- Set up command-line flags
- Set up logging configuration
- Define main execution flow
- Add graceful shutdown handling

‚òê **2.5.2** Implement eBPF loader
- Create loader package
- Load compiled eBPF objects
- Attach programs to tracepoints
- Handle loading errors
- Return collection spec

‚òê **2.5.3** Implement ring buffer reader
- Create reader for each eBPF ring buffer
- Set up event polling
- Parse raw events to Go structures
- Handle partial reads
- Implement error recovery

‚òê **2.5.4** Implement Kubernetes metadata enrichment
- Create Kubernetes client
- Set up pod informer/watcher
- Build PID to pod mapping
- Extract pod labels and annotations
- Cache pod metadata
- Update cache on pod changes

‚òê **2.5.5** Implement cgroup monitoring filter
- Watch for pods with monitoring label
- Extract cgroup ID from pod
- Update eBPF cgroup map
- Remove cgroups when pods terminate
- Handle monitoring label changes

‚òê **2.5.6** Implement event processor
- Create event processing pipeline
- Enrich events with pod metadata
- Extract agent ID from labels
- Add container ID, pod name, namespace
- Prepare events for serialization

‚òê **2.5.7** Implement Kafka producer
- Create Kafka producer client
- Set up connection to Kafka
- Define Avro schema for events
- Serialize events to Avro
- Produce events to topic
- Handle producer errors
- Implement retry logic

‚òê **2.5.8** Add metrics and monitoring
- Add Prometheus metrics for events processed
- Add metrics for errors
- Add metrics for Kafka produce latency
- Expose metrics endpoint

‚òê **2.5.9** Build and package collector
- Build Go binary
- Create Dockerfile for collector
- Build Docker image
- Test image locally
- Push image to registry

#### Success Criteria:
üîç **Verification Steps:**
- Collector binary compiles successfully
- Can load all eBPF programs
- Ring buffer reading works
- Kubernetes client connects to cluster
- Events are enriched with metadata
- Events are produced to Kafka
- Metrics endpoint returns data

üìä **Expected Outputs:**
- Collector binary created
- Docker image builds successfully
- Running collector logs show events
- Kafka topic receives messages
- Metrics show processed event count
- No errors in collector logs

---

### Task 2.6: Deploy eBPF Collector as DaemonSet

#### Subtasks:

‚òê **2.6.1** Create Kubernetes DaemonSet manifest
- Create YAML file for DaemonSet
- Set namespace to agentguard-system
- Configure pod template
- Add app and component labels
- Set update strategy

‚òê **2.6.2** Configure security context
- Set privileged mode to true
- Add required capabilities (SYS_ADMIN, SYS_RESOURCE)
- Mount host PID namespace
- Mount host network namespace
- Configure SELinux context if needed

‚òê **2.6.3** Configure volume mounts
- Mount /sys directory read-only
- Mount /sys/fs/bpf for BPF maps
- Mount /sys/kernel/debug for tracing
- Mount /proc for process information
- Configure volume host paths

‚òê **2.6.4** Configure environment variables
- Set Kafka broker addresses
- Set Kafka topic name
- Set node name from field reference
- Set log level
- Set metrics port

‚òê **2.6.5** Configure resource limits
- Set CPU requests and limits
- Set memory requests and limits
- Verify limits are appropriate
- Document resource requirements

‚òê **2.6.6** Deploy DaemonSet
- Apply DaemonSet manifest
- Monitor rollout status
- Wait for pods to be ready
- Check pod distribution across nodes

‚òê **2.6.7** Verify deployment
- List DaemonSet pods
- Check all pods are Running
- Check one pod per node
- View pod logs
- Verify eBPF programs are loaded
- Check no errors in logs

‚òê **2.6.8** Test event collection
- Deploy test pod with monitoring label
- Perform file operations in test pod
- Check collector captures events
- Verify events appear in Kafka
- Query TimescaleDB for events
- Remove test pod

#### Success Criteria:
üîç **Verification Steps:**
- DaemonSet is successfully created
- One collector pod per node
- All pods are Running
- eBPF programs load without errors
- Events are captured from labeled pods
- Events flow to Kafka successfully
- Events are stored in TimescaleDB

üìä **Expected Outputs:**
- DaemonSet shows desired count equals ready count
- Pod logs show "eBPF programs loaded successfully"
- Kafka consumer shows events flowing
- TimescaleDB query returns captured events
- Metrics endpoint shows event counts
- No permission errors in logs

---

### Task 2.7: Implement Event Consumer and Database Writer

#### Subtasks:

‚òê **2.7.1** Create event consumer service
- Create new Go service in infrastructure directory
- Initialize Go module
- Add Kafka consumer library
- Add PostgreSQL driver
- Set up main structure

‚òê **2.7.2** Implement Kafka consumer
- Create consumer group
- Subscribe to events topic
- Set up message polling
- Handle message deserialization
- Implement offset management

‚òê **2.7.3** Implement database writer
- Create database connection pool
- Prepare insert statement
- Batch events for insertion
- Handle insertion errors
- Implement retry logic

‚òê **2.7.4** Add compression and optimization
- Implement batch buffering
- Add flush interval
- Optimize insert performance
- Handle backpressure

‚òê **2.7.5** Build and deploy consumer
- Build consumer binary
- Create Dockerfile
- Build Docker image
- Create Kubernetes Deployment manifest
- Deploy to agentguard-system namespace

‚òê **2.7.6** Verify consumer operation
- Check consumer pod is Running
- View consumer logs
- Verify consumer is processing messages
- Check database for new events
- Verify data integrity

#### Success Criteria:
üîç **Verification Steps:**
- Consumer pod is Running
- Consumer joins Kafka consumer group
- Messages are consumed from topic
- Events are written to database
- No duplicate events in database
- Consumer handles failures gracefully

üìä **Expected Outputs:**
- Consumer logs show "Consuming from topic"
- Kafka lag is near zero
- Database event count increases
- Query returns events with all fields
- No errors in consumer logs

---

### PHASE 2 COMPLETION CHECKLIST

**Before proceeding to Phase 3, verify ALL of the following:**

‚úÖ eBPF development environment is set up
‚úÖ File monitoring eBPF program compiles and loads
‚úÖ Network monitoring eBPF program compiles and loads
‚úÖ Process monitoring eBPF program compiles and loads
‚úÖ Go collector compiles successfully
‚úÖ Collector can load all eBPF programs
‚úÖ Kubernetes metadata enrichment works
‚úÖ Events are produced to Kafka topic
‚úÖ DaemonSet deploys one pod per node
‚úÖ All collector pods are Running
‚úÖ eBPF programs attach without errors
‚úÖ Consumer service processes events
‚úÖ Events are stored in TimescaleDB
‚úÖ Can query events from database

**Expected State:**
- Kernel-level observability is operational
- All syscalls are captured and enriched
- Event pipeline flows end-to-end
- Database contains real-time events
- Ready to build security decision engine

**Full Pipeline Test:**
- Deploy test agent with monitoring label
- Execute file, network, and process operations
- Verify eBPF collector captures all events
- Confirm events appear in Kafka topic
- Query TimescaleDB and see events
- Verify metadata is correctly enriched

---

## Phase 3: Security Brain (Policy and ML)

**Goal**: Build intelligent security decision engine

**Prerequisites**: Phase 2 completed successfully

**Estimated Complexity**: Advanced

---

### Task 3.1: Set Up OPA Policy Engine

#### Subtasks:

‚òê **3.1.1** Create OPA directory structure
- Create opa-policies directory under security-brain
- Create subdirectories for each policy category
- Create test directory for policy tests
- Create documentation directory

‚òê **3.1.2** Install OPA development tools
- Install OPA binary locally
- Install VS Code OPA extension
- Configure OPA settings
- Verify OPA installation

‚òê **3.1.3** Create base policy structure
- Create main policy package
- Define helper functions package
- Create data imports structure
- Document policy architecture

‚òê **3.1.4** Deploy OPA to Kubernetes
- Create OPA Deployment manifest
- Configure replicas for high availability
- Set resource requests and limits
- Add health check probes
- Deploy to agentguard-system namespace

‚òê **3.1.5** Configure policy loading
- Create ConfigMap for policies
- Mount ConfigMap to OPA pod
- Configure OPA to watch for updates
- Verify policy hot-reload works

‚òê **3.1.6** Verify OPA deployment
- Check OPA pods are Running
- Access OPA API endpoint
- Verify policies are loaded
- Test health endpoint
- Check OPA logs

#### Success Criteria:
üîç **Verification Steps:**
- OPA binary works locally
- OPA pods are Running in cluster
- Can query OPA API
- Policies are loaded into OPA
- Health check returns success

üìä **Expected Outputs:**
- OPA version command returns version
- OPA pod shows Running status
- Policy list endpoint returns loaded policies
- Health endpoint returns 200 OK
- No errors in OPA logs

---

### Task 3.2: Develop Filesystem Protection Policies

#### Subtasks:

‚òê **3.2.1** Create filesystem policy file
- Create filesystem.rego file
- Define package namespace
- Import required data
- Add policy documentation

‚òê **3.2.2** Define critical paths list
- List sensitive system files
- Add configuration directories
- Add credential locations
- Add kernel paths
- Document each entry

‚òê **3.2.3** Implement CRITICAL deny rules
- Create rule for /etc/shadow access
- Create rule for /etc/passwd access
- Create rule for SSH key access
- Create rule for Docker socket access
- Create rule for kernel paths
- Add descriptive messages

‚òê **3.2.4** Implement HIGH risk rules
- Create rule for large database reads
- Create rule for sensitive directory access
- Create rule for configuration file writes
- Add risk level indicators

‚òê **3.2.5** Implement MEDIUM risk rules
- Create rule for temporary file exports
- Create rule for unusual path access
- Add monitoring indicators

‚òê **3.2.6** Add helper functions
- Create function to check path prefix
- Create function to extract file extension
- Create function to check file size
- Document helper usage

‚òê **3.2.7** Write policy tests
- Create test for shadow file access (should deny)
- Create test for normal file access (should allow)
- Create test for large database read (should flag)
- Create test for temporary exports (should flag)
- Verify all tests pass

‚òê **3.2.8** Deploy filesystem policies
- Update ConfigMap with filesystem policies
- Reload policies in OPA
- Verify policies are active
- Test policy evaluation

#### Success Criteria:
üîç **Verification Steps:**
- Policy file has no syntax errors
- All tests pass locally
- Policy is loaded in OPA
- Test queries return expected results
- Critical paths are blocked

üìä **Expected Outputs:**
- opa test command shows all tests passing
- OPA policy list includes filesystem package
- Test query for /etc/shadow returns deny
- Test query for normal file returns allow
- Policy evaluation returns within 10ms

---

### Task 3.3: Develop Network Security Policies

#### Subtasks:

‚òê **3.3.1** Create network policy file
- Create network.rego file
- Define package namespace
- Import required data
- Add policy documentation

‚òê **3.3.2** Define trusted destinations
- List allowed LLM API domains
- List internal service domains
- List approved third-party services
- Document allowlist rationale

‚òê **3.3.3** Implement connection validation
- Create helper for DNS lookup
- Create helper for IP allowlist check
- Create helper for domain matching
- Test helper functions

‚òê **3.3.4** Implement CRITICAL deny rules
- Create rule for connections to untrusted IPs
- Create rule for suspicious port usage
- Create rule for protocol violations
- Add descriptive messages

‚òê **3.3.5** Implement HIGH risk rules
- Create rule for large data transfers
- Create rule for unusual protocols
- Create rule for multiple connections
- Add risk indicators

‚òê **3.3.6** Implement MEDIUM risk rules
- Create rule for non-allowlist HTTPS
- Create rule for daytime vs nighttime traffic
- Add monitoring flags

‚òê **3.3.7** Write policy tests
- Create test for trusted connection (should allow)
- Create test for untrusted connection (should deny)
- Create test for large transfer (should flag)
- Verify all tests pass

‚òê **3.3.8** Deploy network policies
- Update ConfigMap with network policies
- Reload policies in OPA
- Verify policies are active
- Test policy evaluation

#### Success Criteria:
üîç **Verification Steps:**
- Network policy has no syntax errors
- All tests pass locally
- Policy is loaded in OPA
- Allowlist is correctly enforced
- Unknown IPs are flagged

üìä **Expected Outputs:**
- opa test shows all network tests passing
- Trusted domain connection returns allow
- Unknown IP connection returns deny
- Large transfer returns high risk flag
- Policy evaluation is fast

---

### Task 3.4: Develop Tool Governance Policies

#### Subtasks:

‚òê **3.4.1** Create tools policy file
- Create tools.rego file
- Define package namespace
- Import required data
- Add policy documentation

‚òê **3.4.2** Implement SQL injection detection
- Create rule for DROP statements
- Create rule for DELETE without WHERE
- Create rule for TRUNCATE statements
- Create rule for SQL comment injection
- Create rule for UNION attacks
- Test detection rules

‚òê **3.4.3** Implement shell command protection
- Create rule for rm -rf
- Create rule for dd commands
- Create rule for disk formatting
- Create rule for fork bombs
- Create rule for curl pipe to bash
- Test detection rules

‚òê **3.4.4** Implement email protection
- Create rule for external domain emails
- Create rule for mass email detection
- Create rule for attachment size limits
- Test detection rules

‚òê **3.4.5** Implement context-aware rules
- Create rule for database + network correlation
- Create rule for file + network correlation
- Create rule for time-based patterns
- Test correlation logic

‚òê **3.4.6** Write comprehensive tests
- Test each SQL injection pattern
- Test each shell command pattern
- Test email rules
- Test correlation rules
- Verify all tests pass

‚òê **3.4.7** Deploy tool policies
- Update ConfigMap with tool policies
- Reload policies in OPA
- Verify policies are active
- Test policy evaluation

#### Success Criteria:
üîç **Verification Steps:**
- Tools policy has no syntax errors
- All tests pass locally
- SQL injection patterns are detected
- Dangerous commands are blocked
- Correlation rules work correctly

üìä **Expected Outputs:**
- All policy tests pass
- DROP TABLE query returns critical
- rm -rf command returns critical
- External email returns medium risk
- Correlation pattern is detected

---

### Task 3.5: Set Up ML Training Environment

#### Subtasks:

‚òê **3.5.1** Create ML directory structure
- Create ml-models directory under security-brain
- Create training subdirectory
- Create inference subdirectory
- Create data subdirectory
- Create notebooks subdirectory

‚òê **3.5.2** Set up Python environment
- Create Python virtual environment
- Create requirements.txt file
- Install PyTorch
- Install NumPy and Pandas
- Install scikit-learn
- Install Jupyter
- Verify installations

‚òê **3.5.3** Collect training data
- Export benign agent executions from TimescaleDB
- Export labeled attack scenarios
- Create data loading scripts
- Verify data quality
- Split into train/validation/test sets

‚òê **3.5.4** Create data preprocessing pipeline
- Implement event sequence extraction
- Implement feature engineering
- Create event embeddings
- Create temporal features
- Normalize features
- Save preprocessing artifacts

‚òê **3.5.5** Document ML pipeline
- Document data collection process
- Document feature engineering
- Document preprocessing steps
- Add to project documentation

#### Success Criteria:
üîç **Verification Steps:**
- Virtual environment activates
- All packages install successfully
- Can load data from database
- Preprocessing pipeline runs
- Features are correctly generated

üìä **Expected Outputs:**
- requirements.txt lists all dependencies
- Training data files exist
- Preprocessing script runs without errors
- Feature vectors are generated
- Train/val/test splits are balanced

---

### Task 3.6: Train LSTM Anomaly Detection Model

#### Subtasks:

‚òê **3.6.1** Create model architecture
- Define LSTM model class
- Implement embedding layer
- Implement LSTM layers with dropout
- Implement dense output layer
- Document architecture

‚òê **3.6.2** Create training script
- Implement data loaders
- Define loss function (Binary Cross Entropy)
- Define optimizer (Adam)
- Implement training loop
- Implement validation loop
- Add early stopping
- Add model checkpointing

‚òê **3.6.3** Run training
- Start training process
- Monitor training metrics
- Monitor validation metrics
- Wait for convergence
- Save best model checkpoint

‚òê **3.6.4** Evaluate model performance
- Load best model checkpoint
- Run evaluation on test set
- Calculate precision and recall
- Calculate F1 score
- Calculate AUC-ROC
- Generate confusion matrix

‚òê **3.6.5** Tune hyperparameters (if needed)
- Adjust learning rate
- Adjust hidden layer size
- Adjust dropout rate
- Adjust sequence length
- Re-train with new parameters

‚òê **3.6.6** Save final model
- Save model weights
- Save preprocessing artifacts
- Save model configuration
- Document model version
- Add model card documentation

#### Success Criteria:
üîç **Verification Steps:**
- Training completes without errors
- Validation loss decreases over time
- Model achieves >85% precision
- Model achieves >80% recall
- No overfitting observed

üìä **Expected Outputs:**
- Training logs show decreasing loss
- Final validation loss is low
- Test set F1 score is >0.85
- Model file is saved
- Can load model and make predictions

---

### Task 3.7: Build ML Model Inference Service

#### Subtasks:

‚òê **3.7.1** Create inference service structure
- Create Flask application file
- Define API endpoints
- Set up error handling
- Configure CORS if needed

‚òê **3.7.2** Implement model loading
- Load PyTorch model at startup
- Load preprocessing artifacts
- Verify model loads correctly
- Set model to evaluation mode

‚òê **3.7.3** Implement prediction endpoint
- Create POST endpoint for predictions
- Parse input event sequences
- Preprocess input data
- Run model inference
- Calculate anomaly score
- Determine risk level
- Return JSON response

‚òê **3.7.4** Add health endpoint
- Create GET endpoint for health check
- Verify model is loaded
- Return service status

‚òê **3.7.5** Add metrics endpoint
- Create endpoint for Prometheus metrics
- Track prediction count
- Track latency
- Track errors

‚òê **3.7.6** Build and deploy inference service
- Create Dockerfile
- Build Docker image
- Push to registry
- Create Kubernetes Deployment
- Deploy to agentguard-system namespace
- Create Service resource

‚òê **3.7.7** Test inference service
- Send test prediction request
- Verify response format
- Check anomaly score is reasonable
- Verify latency is acceptable
- Check metrics endpoint

#### Success Criteria:
üîç **Verification Steps:**
- Inference service pod is Running
- Health endpoint returns success
- Prediction endpoint returns scores
- Latency is under 50ms P99
- Metrics are exposed

üìä **Expected Outputs:**
- Service pod shows Running status
- Health check returns 200 OK
- Test prediction returns anomaly score
- Score is between 0 and 1
- Metrics show prediction count

---

### Task 3.8: Implement LLM Observer with Claude

#### Subtasks:

‚òê **3.8.1** Create LLM observer service
- Create new Python service
- Initialize Flask application
- Install Anthropic SDK
- Set up API key configuration

‚òê **3.8.2** Create analysis prompt template
- Define prompt structure
- Include agent context section
- Include recent events section
- Include current action section
- Include analysis questions
- Define expected JSON response format

‚òê **3.8.3** Implement Claude API integration
- Create Anthropic client
- Implement prompt construction
- Implement API call with error handling
- Implement response parsing
- Handle rate limiting
- Add retry logic

‚òê **3.8.4** Create analysis endpoint
- Define POST endpoint
- Parse input request
- Construct analysis prompt
- Call Claude API
- Parse Claude response
- Return risk assessment

‚òê **3.8.5** Add caching (optional)
- Implement response cache for similar queries
- Set cache expiration
- Add cache hit metrics

‚òê **3.8.6** Build and deploy LLM observer
- Create Dockerfile
- Build Docker image
- Create Kubernetes Secret for API key
- Create Deployment manifest
- Deploy to cluster
- Create Service resource

‚òê **3.8.7** Test LLM observer
- Send test analysis request
- Verify Claude response
- Check reasoning quality
- Verify JSON parsing
- Check latency

#### Success Criteria:
üîç **Verification Steps:**
- LLM observer pod is Running
- Can connect to Anthropic API
- Analysis returns structured JSON
- Reasoning is coherent
- Risk assessment is reasonable

üìä **Expected Outputs:**
- Pod shows Running status
- API key secret is mounted
- Test request returns analysis
- Response includes risk score
- Response includes reasoning
- Latency is under 2 seconds

---

### Task 3.9: Build Security Brain Decision Engine

#### Subtasks:

‚òê **3.9.1** Create decision engine service
- Create new Go service
- Set up HTTP server
- Define API endpoints
- Configure dependencies

‚òê **3.9.2** Implement OPA client
- Create client for OPA HTTP API
- Implement policy query method
- Parse OPA response
- Handle OPA errors

‚òê **3.9.3** Implement ML client
- Create client for ML inference service
- Implement prediction request method
- Parse ML response
- Handle ML service errors

‚òê **3.9.4** Implement LLM client
- Create client for LLM observer service
- Implement analysis request method
- Parse LLM response
- Handle LLM service errors
- Implement async mode for ALLOW decisions

‚òê **3.9.5** Implement decision fusion logic
- Create risk scoring aggregation
- Implement weighted combination
- Define decision thresholds
- Implement decision rules
- Handle conflicting signals

‚òê **3.9.6** Create assessment endpoint
- Define POST endpoint for risk assessment
- Parse incoming event context
- Call OPA, ML, and LLM in parallel
- Aggregate results
- Make final decision
- Return decision with reasoning

‚òê **3.9.7** Add decision logging
- Log all decisions to Kafka
- Include all component scores
- Include final decision
- Include latency metrics

‚òê **3.9.8** Build and deploy decision engine
- Build Go binary
- Create Dockerfile
- Build image
- Create Deployment manifest
- Deploy to cluster
- Create Service resource

‚òê **3.9.9** Test decision engine
- Send benign event (should ALLOW)
- Send malicious event (should BLOCK)
- Send borderline event (should HITL)
- Verify decision logic
- Check latency

#### Success Criteria:
üîç **Verification Steps:**
- Decision engine pod is Running
- Can connect to all components
- Parallel queries work correctly
- Decision fusion is logical
- Latency is under 50ms for ALLOW

üìä **Expected Outputs:**
- Pod shows Running status
- Benign request returns ALLOW
- Malicious request returns BLOCK
- Response includes all component scores
- Latency meets targets
- Decisions are logged to Kafka

---

### PHASE 3 COMPLETION CHECKLIST

**Before proceeding to Phase 4, verify ALL of the following:**

‚úÖ OPA is deployed and running
‚úÖ Filesystem policies are loaded and working
‚úÖ Network policies are loaded and working
‚úÖ Tool governance policies are loaded and working
‚úÖ All policy tests pass
‚úÖ ML training environment is set up
‚úÖ LSTM model is trained successfully
‚úÖ Model achieves target performance metrics
‚úÖ ML inference service is deployed
‚úÖ LLM observer service is deployed
‚úÖ Can query Claude API successfully
‚úÖ Security Brain decision engine is deployed
‚úÖ All components can communicate
‚úÖ Decision fusion works correctly
‚úÖ Can make ALLOW, BLOCK, and HITL decisions

**Expected State:**
- Intelligent security decision system operational
- Multi-layer defense architecture working
- All components integrated
- Ready to add human oversight

**Full Security Brain Test:**
- Send benign file access event (expect ALLOW from all)
- Send /etc/shadow access (expect CRITICAL from OPA)
- Send data exfiltration pattern (expect HIGH from ML)
- Send prompt injection (expect HIGH from LLM)
- Verify decision fusion aggregates correctly
- Check latency is acceptable

---

## Phase 4: Human-in-the-Loop Workflows

**Goal**: Implement human approval system for high-risk operations

**Prerequisites**: Phase 3 completed successfully

**Estimated Complexity**: Intermediate

---

### Task 4.1: Build Tool Proxy Service

#### Subtasks:

‚òê **4.1.1** Create tool proxy project
- Create new Go project in tool-proxy directory
- Initialize Go module
- Add required dependencies
- Set up project structure

‚òê **4.1.2** Define tool execution interface
- Create tool interface definition
- Define execution request structure
- Define execution response structure
- Document tool contract

‚òê **4.1.3** Implement HTTP proxy server
- Create HTTP server
- Define route for tool execution
- Parse incoming requests
- Validate request format

‚òê **4.1.4** Integrate with Security Brain
- Create client for Security Brain API
- Send tool request for risk assessment
- Receive decision (ALLOW/BLOCK/HITL)
- Handle decision appropriately

‚òê **4.1.5** Implement ALLOW path
- Execute tool directly
- Capture tool output
- Return result to agent
- Log execution

‚òê **4.1.6** Implement BLOCK path
- Return error to agent
- Include block reason
- Include incident ID
- Log blocked attempt

‚òê **4.1.7** Implement HITL path
- Generate unique request ID
- Store request in Redis queue
- Start Temporal workflow
- Wait for decision signal
- Return workflow result

‚òê **4.1.8** Add timeout handling
- Implement request timeout
- Handle workflow timeout
- Return timeout error
- Clean up resources

‚òê **4.1.9** Build and deploy tool proxy
- Build Go binary
- Create Dockerfile
- Build image
- Create Deployment manifest
- Deploy to cluster
- Create Service resource

‚òê **4.1.10** Test tool proxy
- Send test tool request
- Verify ALLOW path works
- Verify BLOCK path works
- Verify HITL path creates workflow

#### Success Criteria:
üîç **Verification Steps:**
- Tool proxy pod is Running
- Can accept tool execution requests
- Security Brain integration works
- ALLOW requests execute immediately
- BLOCK requests return errors
- HITL requests create workflows

üìä **Expected Outputs:**
- Pod shows Running status
- ALLOW request returns tool result
- BLOCK request returns error with reason
- HITL request returns pending status
- Requests are logged correctly

---

### Task 4.2: Implement Temporal Workflow Definitions

#### Subtasks:

‚òê **4.2.1** Set up Temporal worker project
- Create worker directory in tool-proxy
- Initialize Go module
- Add Temporal SDK dependency
- Set up worker structure

‚òê **4.2.2** Define workflow interface
- Create approval workflow interface
- Define workflow input structure
- Define workflow output structure
- Document workflow contract

‚òê **4.2.3** Implement approval workflow
- Create HumanApprovalWorkflow function
- Parse workflow input
- Store approval request in Redis
- Send notifications
- Wait for approval signal
- Handle timeout
- Execute tool on approval
- Return result

‚òê **4.2.4** Implement notification activity
- Create SendNotifications activity
- Implement Slack notification
- Implement email notification
- Implement PagerDuty for CRITICAL
- Handle notification failures

‚òê **4.2.5** Implement tool execution activity
- Create ExecuteToolCall activity
- Execute actual tool
- Capture output
- Handle execution errors
- Return result

‚òê **4.2.6** Implement logging activity
- Create LogApprovalDecision activity
- Write to Kafka
- Write to database
- Include all metadata

‚òê **4.2.7** Register workflow and activities
- Create Temporal worker
- Register HumanApprovalWorkflow
- Register all activities
- Configure worker options
- Start worker

‚òê **4.2.8** Build and deploy worker
- Build worker binary
- Create Dockerfile
- Build image
- Create Deployment manifest
- Deploy to cluster

‚òê **4.2.9** Test workflow execution
- Trigger test workflow
- Verify workflow starts
- Check workflow state
- Verify activities execute
- Test signal handling

#### Success Criteria:
üîç **Verification Steps:**
- Worker pod is Running
- Worker connects to Temporal
- Workflows can be started
- Activities execute successfully
- Signals are received
- Timeouts work correctly

üìä **Expected Outputs:**
- Worker pod shows Running status
- Temporal UI shows workflow registered
- Test workflow appears in UI
- Workflow progresses through activities
- Can send signal to workflow
- Workflow completes successfully

---

### Task 4.3: Set Up Redis Approval Queue

#### Subtasks:

‚òê **4.3.1** Verify Redis deployment
- Check Redis is running from Phase 1
- Verify connection works
- Test basic operations

‚òê **4.3.2** Define queue data structure
- Design approval request structure
- Define Redis key naming
- Define expiration policy
- Document data model

‚òê **4.3.3** Implement queue operations
- Create function to add to queue
- Create function to get from queue
- Create function to remove from queue
- Create function to list pending
- Add to tool proxy service

‚òê **4.3.4** Test queue operations
- Add test approval request
- List pending requests
- Retrieve specific request
- Remove completed request
- Verify operations work

#### Success Criteria:
üîç **Verification Steps:**
- Can connect to Redis
- Can add items to queue
- Can retrieve items from queue
- Can list all pending items
- Expiration works correctly

üìä **Expected Outputs:**
- Redis GET returns stored data
- Queue list returns all pending items
- Expired items are removed automatically
- Data structure is consistent

---

### Task 4.4: Build Web UI Dashboard

#### Subtasks:

‚òê **4.4.1** Set up Next.js project
- Create web-ui directory
- Initialize Next.js project
- Install required dependencies
- Configure TypeScript
- Set up Tailwind CSS

‚òê **4.4.2** Create project structure
- Create pages directory
- Create components directory
- Create lib directory for utilities
- Create styles directory
- Set up API routes

‚òê **4.4.3** Implement authentication (basic)
- Set up session management
- Create login page
- Implement auth middleware
- Configure protected routes

‚òê **4.4.4** Create API routes
- Create route for pending approvals
- Create route for approval action
- Create route for rejection action
- Create route for agent statistics

‚òê **4.4.5** Implement pending approvals page
- Create approvals page
- Fetch pending approvals from API
- Display in categorized lists (CRITICAL, HIGH, MEDIUM)
- Show request details
- Add auto-refresh

‚òê **4.4.6** Create approval card component
- Design approval card layout
- Show agent ID and tool
- Show operation details
- Show risk scores
- Display timeout countdown
- Add approve/reject buttons

‚òê **4.4.7** Implement approval action
- Create approve button handler
- Prompt for justification
- Send approval signal to Temporal
- Update UI
- Show success message

‚òê **4.4.8** Implement rejection action
- Create reject button handler
- Prompt for reason
- Send rejection signal to Temporal
- Update UI
- Show success message

‚òê **4.4.9** Create detail modal
- Design full-detail view
- Show complete request context
- Show recent agent events
- Show risk analysis
- Add timeline view

‚òê **4.4.10** Add real-time updates
- Implement Server-Sent Events or polling
- Update approval list automatically
- Show notifications for new requests
- Update countdown timers

‚òê **4.4.11** Build and deploy web UI
- Build production bundle
- Create Dockerfile
- Build image
- Create Deployment manifest
- Create Service resource
- Deploy to cluster

‚òê **4.4.12** Set up ingress (optional)
- Create Ingress resource
- Configure hostname
- Set up TLS certificate
- Test external access

#### Success Criteria:
üîç **Verification Steps:**
- Web UI pod is Running
- Can access dashboard in browser
- Login works
- Can see pending approvals
- Can approve requests
- Can reject requests
- Real-time updates work

üìä **Expected Outputs:**
- Dashboard loads without errors
- Pending approvals are displayed
- Can click approve and see workflow complete
- Can click reject and see workflow fail
- UI updates when new requests arrive
- Styling looks professional

---

### Task 4.5: Implement Notification Channels

#### Subtasks:

‚òê **4.5.1** Set up Slack integration (optional)
- Create Slack webhook
- Store webhook URL in Secret
- Test webhook connectivity
- Implement Slack message formatting

‚òê **4.5.2** Implement Slack notification
- Create Slack client
- Format approval request as message
- Add action buttons (if using interactive messages)
- Send message to channel
- Handle errors

‚òê **4.5.3** Set up email integration (optional)
- Configure SMTP settings
- Create email template
- Implement email sending
- Test email delivery

‚òê **4.5.4** Set up PagerDuty integration (optional)
- Create PagerDuty integration key
- Store key in Secret
- Implement incident creation
- Test incident triggering

‚òê **4.5.5** Configure notification routing
- Define notification rules
- Route CRITICAL to all channels
- Route HIGH to Slack/Email
- Route MEDIUM to email only
- Document routing logic

‚òê **4.5.6** Test all notification channels
- Trigger CRITICAL approval
- Verify all channels receive notification
- Trigger HIGH approval
- Verify Slack and email receive notification
- Check notification content

#### Success Criteria:
üîç **Verification Steps:**
- Slack messages are received
- Emails are received
- PagerDuty incidents are created
- Notifications include all details
- Routing works correctly

üìä **Expected Outputs:**
- Slack channel shows approval request
- Email inbox has approval notification
- PagerDuty shows open incident
- All notifications are properly formatted
- No errors in logs

---

### Task 4.6: End-to-End HITL Testing

#### Subtasks:

‚òê **4.6.1** Deploy test agent
- Create test agent with monitoring label
- Deploy to agentguard-agents namespace
- Verify agent is running
- Verify eBPF is monitoring

‚òê **4.6.2** Trigger HITL workflow
- Configure tool call to require approval
- Execute tool call from agent
- Verify request is sent to tool proxy
- Verify Security Brain returns HITL
- Verify workflow starts

‚òê **4.6.3** Verify approval request appears
- Check Redis queue has request
- Check web UI shows pending approval
- Check notifications are sent
- Verify all details are correct

‚òê **4.6.4** Test approval path
- Click approve in web UI
- Provide justification
- Verify signal sent to Temporal
- Verify tool executes
- Verify result returned to agent
- Check audit log

‚òê **4.6.5** Test rejection path
- Trigger new HITL request
- Click reject in web UI
- Provide reason
- Verify signal sent to Temporal
- Verify error returned to agent
- Check audit log

‚òê **4.6.6** Test timeout path
- Trigger HITL request
- Wait for timeout period
- Verify auto-rejection occurs
- Verify error returned to agent
- Check timeout is logged

‚òê **4.6.7** Verify audit trail
- Query approval logs from database
- Verify all decisions are recorded
- Check reviewer information is captured
- Verify timestamps are correct
- Check all metadata is present

#### Success Criteria:
üîç **Verification Steps:**
- Complete flow works end-to-end
- Approval executes tool successfully
- Rejection returns error to agent
- Timeout auto-rejects after SLA
- All decisions are logged

üìä **Expected Outputs:**
- Agent receives tool result on approval
- Agent receives error on rejection
- Timeout triggers after configured time
- Database has complete audit trail
- All workflows complete successfully

---

### PHASE 4 COMPLETION CHECKLIST

**Before proceeding to Phase 5, verify ALL of the following:**

‚úÖ Tool proxy service is deployed and running
‚úÖ Can route requests based on Security Brain decision
‚úÖ Temporal worker is deployed and registered
‚úÖ Workflows and activities are working
‚úÖ Redis queue stores pending approvals
‚úÖ Web UI dashboard is accessible
‚úÖ Can view pending approvals in dashboard
‚úÖ Can approve operations from dashboard
‚úÖ Can reject operations from dashboard
‚úÖ Notifications are sent correctly
‚úÖ ALLOW path executes immediately
‚úÖ BLOCK path returns errors
‚úÖ HITL path waits for human decision
‚úÖ Timeouts work correctly
‚úÖ Complete audit trail exists

**Expected State:**
- Human oversight system fully operational
- High-risk operations require approval
- Users can review and decide
- Complete audit trail maintained
- Ready to add adversarial testing

**Full HITL System Test:**
- Configure agent to use tool proxy
- Execute benign operation (ALLOW - instant)
- Execute dangerous operation (BLOCK - immediate error)
- Execute borderline operation (HITL - approval required)
- Approve from dashboard
- Verify tool executes
- Check audit log has complete record

---

## Phase 5: Red Team Engine and Hardening

**Goal**: Build automated adversarial testing and policy generation

**Prerequisites**: Phase 4 completed successfully

**Estimated Complexity**: Advanced

---

### Task 5.1: Set Up Red Team Environment

#### Subtasks:

‚òê **5.1.1** Create red-team project structure
- Create red-team directory
- Create attack-generator subdirectory
- Create executor subdirectory
- Create analyzer subdirectory
- Create reports subdirectory

‚òê **5.1.2** Set up Python environment for red team
- Create virtual environment
- Create requirements.txt
- Install LangChain/LangGraph
- Install Anthropic SDK
- Install data analysis libraries
- Verify installations

‚òê **5.1.3** Create attack taxonomy
- Define attack categories
- Document attack types
- Create attack template structure
- Define success criteria
- Document taxonomy

‚òê **5.1.4** Set up honeytoken system
- Design honeytoken strategy
- Create honeytoken insertion script
- Insert honeytokens in test database
- Document honeytoken locations
- Create detection mechanism

‚òê **5.1.5** Document red team approach
- Document attack generation methodology
- Document execution approach
- Document success detection
- Add to project documentation

#### Success Criteria:
üîç **Verification Steps:**
- Python environment is set up
- All libraries install successfully
- Attack taxonomy is documented
- Honeytokens are deployed
- Detection mechanism works

üìä **Expected Outputs:**
- Virtual environment activates
- Requirements install without errors
- Taxonomy document is complete
- Database contains honeytokens
- Can detect honeytoken access

---

### Task 5.2: Build Attack Generation Agent

#### Subtasks:

‚òê **5.2.1** Create LangGraph workflow structure
- Define attack generation graph
- Create state schema
- Define workflow nodes
- Connect nodes with edges
- Set entry and end points

‚òê **5.2.2** Implement target analysis node
- Create function to analyze blue agent
- Parse agent specification
- Identify attack surfaces
- Extract tool capabilities
- Return analysis results

‚òê **5.2.3** Implement attack generation node
- Create Claude prompting for attack generation
- Include attack taxonomy
- Include agent context
- Request diverse scenarios
- Parse JSON response
- Return attack list

‚òê **5.2.4** Implement attack categorization
- Sort attacks by category
- Sort by difficulty
- Sort by target component
- Create attack manifest

‚òê **5.2.5** Test attack generator
- Run against sample agent spec
- Verify diverse attacks generated
- Check attack quality
- Validate JSON format
- Review attack coverage

‚òê **5.2.6** Save generated attacks
- Serialize attacks to JSON
- Save to file system
- Version attack sets
- Document attack generation

#### Success Criteria:
üîç **Verification Steps:**
- LangGraph workflow executes
- Attacks are generated successfully
- Attacks cover all categories
- JSON format is valid
- Attacks are diverse

üìä **Expected Outputs:**
- Workflow completes without errors
- JSON file contains 50+ attacks
- All attack categories are represented
- Each attack has complete structure
- Attacks are reasonable and executable

---

### Task 5.3: Build Attack Execution Framework

#### Subtasks:

‚òê **5.3.1** Create test environment deployer
- Implement function to deploy test agent
- Configure with AgentGuard enabled
- Set monitoring labels
- Wait for deployment ready
- Return agent endpoint

‚òê **5.3.2** Create attack executor
- Implement function to send attack prompt
- Capture agent response
- Monitor syscalls during execution
- Monitor tool calls
- Capture AgentGuard decision
- Record timeline

‚òê **5.3.3** Implement success detection
- Check for honeytoken in response
- Check for forbidden syscalls
- Check for policy violations
- Check for network exfiltration
- Classify attack result

‚òê **5.3.4** Implement attack runner
- Load attack scenarios
- Execute each attack sequentially
- Collect results
- Handle errors
- Clean up between attacks

‚òê **5.3.5** Test execution framework
- Deploy test blue agent
- Execute sample attack
- Verify monitoring works
- Verify success detection
- Clean up test agent

#### Success Criteria:
üîç **Verification Steps:**
- Can deploy test agents
- Can execute attack prompts
- Syscalls are captured
- Success detection works
- Cleanup removes test resources

üìä **Expected Outputs:**
- Test agent deploys successfully
- Attack prompt is sent
- Response is captured
- Syscall trace is available
- Success/failure is correctly determined

---

### Task 5.4: Build Results Analysis System

#### Subtasks:

‚òê **5.4.1** Create results aggregator
- Collect all attack results
- Calculate success rate
- Group by category
- Group by defense layer
- Calculate statistics

‚òê **5.4.2** Implement defense attribution
- Identify which layer blocked each attack
- Count blocks per layer
- Calculate attribution percentages
- Generate attribution report

‚òê **5.4.3** Identify successful attacks
- Filter for successful attacks
- Extract attack patterns
- Identify commonalities
- Prioritize by severity

‚òê **5.4.4** Create vulnerability report
- Document successful attacks
- Document bypass methods
- Suggest remediation
- Prioritize fixes

‚òê **5.4.5** Generate HTML report
- Create HTML template
- Populate with results
- Add charts and graphs
- Include attack details
- Include recommendations

‚òê **5.4.6** Test analysis system
- Run on sample results
- Verify calculations
- Check report generation
- Validate HTML output

#### Success Criteria:
üîç **Verification Steps:**
- Results are aggregated correctly
- Statistics are accurate
- Attribution is logical
- Report is comprehensive
- HTML renders properly

üìä **Expected Outputs:**
- Summary shows attack success rate
- Defense attribution percentages add to 100%
- Successful attacks are listed
- HTML report is readable
- Charts display correctly

---

### Task 5.5: Implement Automated Policy Generation

#### Subtasks:

‚òê **5.5.1** Create policy generator
- Create function to analyze successful attack
- Extract syscall patterns
- Identify attack characteristics
- Format for Claude analysis

‚òê **5.5.2** Implement Claude-based policy creation
- Create prompt for policy generation
- Include attack details
- Include syscall trace
- Request OPA Rego policy
- Parse policy response

‚òê **5.5.3** Implement policy validation
- Check Rego syntax
- Verify policy compiles
- Test policy against attack
- Verify policy blocks attack

‚òê **5.5.4** Create policy deployment
- Save generated policies
- Add to auto-generated package
- Update OPA ConfigMap
- Reload policies

‚òê **5.5.5** Test policy generation
- Use sample successful attack
- Generate policy
- Validate policy
- Deploy policy
- Re-run attack
- Verify attack is now blocked

#### Success Criteria:
üîç **Verification Steps:**
- Policy generation completes
- Generated Rego is syntactically valid
- Policy compiles in OPA
- Policy blocks the attack
- Policy is deployable

üìä **Expected Outputs:**
- Claude generates valid Rego code
- OPA policy check passes
- Policy blocks test attack
- Policy is added to ConfigMap
- Can verify policy in OPA

---

### Task 5.6: Implement Closed-Loop Hardening

#### Subtasks:

‚òê **5.6.1** Create hardening workflow
- Define workflow steps
- Run initial attack campaign
- Collect successful attacks
- Generate policies for each
- Validate policies
- Deploy updated policies

‚òê **5.6.2** Implement re-validation
- Re-run same attack campaign
- Compare results to initial run
- Calculate improvement
- Identify remaining gaps

‚òê **5.6.3** Create improvement metrics
- Calculate attack success reduction
- Measure Safety Score improvement
- Measure Utility impact
- Generate before/after comparison

‚òê **5.6.4** Test closed-loop hardening
- Run initial campaign
- Note success rate
- Generate and deploy policies
- Re-run campaign
- Verify reduced success rate

‚òê **5.6.5** Document hardening results
- Document initial results
- Document generated policies
- Document improvement metrics
- Add to project documentation

#### Success Criteria:
üîç **Verification Steps:**
- Initial campaign completes
- Policies are generated
- Policies are deployed
- Re-run shows improvement
- Improvement is measurable

üìä **Expected Outputs:**
- Initial attack success rate (e.g., 12%)
- Generated policies count (e.g., 3 policies)
- New attack success rate (e.g., 5%)
- Improvement percentage (e.g., 58% reduction)
- Documentation is complete

---

### Task 5.7: Build Campaign Orchestrator

#### Subtasks:

‚òê **5.7.1** Create campaign script
- Create main campaign orchestrator
- Parse command-line arguments
- Load configuration
- Set up logging

‚òê **5.7.2** Implement campaign flow
- Load or generate attacks
- Deploy blue agent
- Execute all attacks
- Collect results
- Analyze results
- Generate report
- Optionally generate policies
- Optionally re-run for validation

‚òê **5.7.3** Add progress tracking
- Show progress bar
- Log each attack execution
- Show intermediate results
- Estimate time remaining

‚òê **5.7.4** Add error handling
- Handle deployment failures
- Handle execution errors
- Continue on non-critical errors
- Log all errors
- Generate error report

‚òê **5.7.5** Test campaign orchestrator
- Run small test campaign (10 attacks)
- Verify all steps execute
- Check report generation
- Verify cleanup

‚òê **5.7.6** Run full campaign
- Generate 100 attack scenarios
- Execute full campaign
- Review results
- Generate policies
- Run validation campaign

#### Success Criteria:
üîç **Verification Steps:**
- Campaign executes end-to-end
- All attacks are executed
- Results are accurate
- Report is generated
- Policies are created
- Validation shows improvement

üìä **Expected Outputs:**
- Campaign completes successfully
- 100 attacks executed
- Results summary shows statistics
- HTML report is generated
- Policies are created
- Re-run shows reduced success rate

---

### PHASE 5 COMPLETION CHECKLIST

**Before proceeding to Phase 6, verify ALL of the following:**

‚úÖ Red team environment is set up
‚úÖ Attack taxonomy is documented
‚úÖ Honeytokens are deployed
‚úÖ Attack generator creates diverse scenarios
‚úÖ Attack executor runs attacks successfully
‚úÖ Success detection works correctly
‚úÖ Results analysis aggregates data
‚úÖ HTML reports are generated
‚úÖ Policy generator creates valid Rego
‚úÖ Policies block attacks
‚úÖ Closed-loop hardening works
‚úÖ Campaign orchestrator runs end-to-end
‚úÖ Full campaign completes successfully
‚úÖ Improvement is measurable

**Expected State:**
- Automated adversarial testing operational
- Can generate and execute 100+ attacks
- Can automatically generate policies
- Measurable security improvement
- Ready for comprehensive evaluation

**Full Red Team Test:**
- Generate 50 attack scenarios
- Deploy test agent with AgentGuard
- Execute all attacks
- Verify success detection
- Generate report showing 10-15% success
- Generate policies from successful attacks
- Deploy policies
- Re-run attacks
- Verify success rate drops to 3-5%
- Document 60%+ improvement

---

## Phase 6: Evaluation, Documentation & Production Readiness

**Goal**: Complete evaluation, documentation, and prepare for production

**Prerequisites**: Phase 5 completed successfully

**Estimated Complexity**: Intermediate

---

### Task 6.1: Build Comprehensive Benchmark Suite

#### Subtasks:

‚òê **6.1.1** Create benchmark scenarios
- Create customer support agent scenario
- Create DevOps agent scenario
- Create data analyst agent scenario
- Document each scenario
- Define success criteria

‚òê **6.1.2** Create benign task corpus
- Create 50 benign tasks per scenario
- Cover normal operations
- Test all tools
- Document expected outcomes
- Ensure realistic usage patterns

‚òê **6.1.3** Create adversarial task corpus
- Select diverse attack types
- Cover all threat categories
- Include novel attacks
- Document expected detections
- Ensure comprehensive coverage

‚òê **6.1.4** Create benchmark runner
- Implement scenario executor
- Run benign tasks
- Run adversarial tasks
- Collect metrics
- Generate results

‚òê **6.1.5** Test benchmark suite
- Run against one scenario
- Verify task execution
- Check metric collection
- Validate results format

#### Success Criteria:
üîç **Verification Steps:**
- Three scenarios are documented
- 150 benign tasks created
- 100+ adversarial tasks created
- Benchmark runner executes
- Results are collected

üìä **Expected Outputs:**
- Scenario YAML files exist
- Task corpus is complete
- Runner executes all tasks
- Metrics are captured
- Results JSON is generated

---

### Task 6.2: Run Configuration Matrix Evaluation

#### Subtasks:

‚òê **6.2.1** Define test configurations
- Config A: Baseline (no AgentGuard)
- Config B: eBPF only
- Config C: eBPF + OPA
- Config D: eBPF + OPA + ML
- Config E: eBPF + OPA + ML + LLM
- Config F: Full (+ HITL)
- Config G: Hardened (+ learned policies)

‚òê **6.2.2** Implement configuration switcher
- Create function to enable/disable components
- Implement configuration deployment
- Verify configuration is applied
- Document configuration process

‚òê **6.2.3** Run evaluation for each configuration
- Deploy Config A
- Run all scenarios
- Collect results
- Repeat for Configs B-G

‚òê **6.2.4** Collect comprehensive metrics
- Measure attack success rate
- Measure task success rate
- Measure latency (P50, P95, P99)
- Measure resource usage
- Measure LLM API costs
- Record defense attribution

‚òê **6.2.5** Save all results
- Save per-configuration results
- Save per-scenario results
- Save raw data
- Save aggregated statistics

#### Success Criteria:
üîç **Verification Steps:**
- All configurations tested
- All scenarios executed per config
- Metrics collected for all
- Results are reproducible
- Data is complete

üìä **Expected Outputs:**
- Seven configuration result files
- Each has safety and utility scores
- Latency data is captured
- Cost data is available
- Results are consistent

---

### Task 6.3: Generate Pareto Frontier Analysis

#### Subtasks:

‚òê **6.3.1** Calculate Safety Scores
- For each configuration, calculate attack success rate
- Compute Safety Score = (1 - ASR) √ó 100
- Verify scores are reasonable
- Document calculation

‚òê **6.3.2** Calculate Utility Scores
- Calculate task success rate
- Calculate latency overhead
- Calculate cost multiplier
- Combine with weighted formula
- Compute Utility Score
- Verify scores are reasonable

‚òê **6.3.3** Create Pareto analysis
- Plot Safety vs Utility
- Identify Pareto frontier
- Mark dominated configurations
- Identify optimal operating points

‚òê **6.3.4** Generate visualizations
- Create scatter plot
- Add Pareto frontier line
- Label each configuration
- Add axis labels and title
- Save as PNG/SVG

‚òê **6.3.5** Create trade-off analysis
- Document each configuration's position
- Explain trade-offs
- Recommend configurations for different use cases
- Add to documentation

#### Success Criteria:
üîç **Verification Steps:**
- Safety scores are calculated
- Utility scores are calculated
- Pareto frontier is identified
- Visualization is clear
- Analysis is documented

üìä **Expected Outputs:**
- Each config has Safety and Utility scores
- Scatter plot shows all configurations
- Pareto frontier is visible
- Chart clearly shows trade-offs
- Recommendations are documented

---

### Task 6.4: Run Ablation Studies

#### Subtasks:

‚òê **6.4.1** Isolate eBPF contribution
- Compare Baseline vs eBPF Only
- Calculate attacks caught by eBPF
- Document eBPF effectiveness
- Identify eBPF blind spots

‚òê **6.4.2** Isolate OPA contribution
- Compare eBPF Only vs eBPF + OPA
- Calculate attacks caught by OPA
- Document policy effectiveness
- Identify policy gaps

‚òê **6.4.3** Isolate ML contribution
- Compare (eBPF + OPA) vs (eBPF + OPA + ML)
- Calculate novel attacks caught by ML
- Document ML effectiveness
- Identify ML limitations

‚òê **6.4.4** Isolate LLM contribution
- Compare with and without LLM
- Calculate contextual threats caught
- Document LLM effectiveness
- Measure LLM cost impact

‚òê **6.4.5** Isolate HITL contribution
- Measure safety improvement with HITL
- Measure latency impact
- Document HITL trade-offs

‚òê **6.4.6** Create ablation report
- Compile all ablation results
- Create comparison tables
- Generate visualization
- Document insights

#### Success Criteria:
üîç **Verification Steps:**
- Each component is isolated
- Contribution is measured
- Comparisons are fair
- Insights are documented
- Report is comprehensive

üìä **Expected Outputs:**
- Component contribution percentages
- Comparison tables
- Visualization showing impact
- Clear conclusions
- Documentation is complete

---

### Task 6.5: Complete Technical Documentation

#### Subtasks:

‚òê **6.5.1** Finalize README
- Complete installation instructions
- Add all diagrams
- Verify all links work
- Add screenshots
- Proofread thoroughly

‚òê **6.5.2** Complete architecture documentation
- Document all components
- Create architecture diagrams
- Explain data flows
- Document design decisions
- Add to docs/architecture

‚òê **6.5.3** Complete API documentation
- Document all REST endpoints
- Document GraphQL schema
- Provide example requests/responses
- Add authentication details
- Add to docs/api

‚òê **6.5.4** Complete deployment documentation
- Document local deployment
- Document cloud deployment
- Document configuration options
- Add troubleshooting guide
- Add to docs/deployment

‚òê **6.5.5** Create operator guide
- Document daily operations
- Document monitoring
- Document incident response
- Document maintenance tasks
- Add to docs/operations

‚òê **6.5.6** Create developer guide
- Document how to contribute
- Explain codebase structure
- Document build process
- Explain testing approach
- Add to docs/development

‚òê **6.5.7** Verify all documentation
- Check all code examples work
- Verify all commands are correct
- Test all links
- Check spelling and grammar
- Ensure consistency

#### Success Criteria:
üîç **Verification Steps:**
- All documentation exists
- Examples are tested
- Links are valid
- No spelling errors
- Documentation is clear

üìä **Expected Outputs:**
- README is comprehensive
- Architecture docs are complete
- API docs are accurate
- Deployment docs work
- Operator guide is helpful
- Developer guide is clear

---

### Task 6.6: Prepare Production Deployment

#### Subtasks:

‚òê **6.6.1** Create production manifests
- Update resource limits for production
- Configure replicas for HA
- Add production-grade monitoring
- Configure backup settings
- Add disaster recovery config

‚òê **6.6.2** Create Helm chart
- Create Chart.yaml
- Create values.yaml with all configs
- Create templates for all resources
- Document chart usage
- Test chart installation

‚òê **6.6.3** Configure secrets management
- Document secret creation
- Create secret templates
- Document rotation procedures
- Add security best practices

‚òê **6.6.4** Set up monitoring dashboards
- Create Grafana dashboards
- Add all key metrics
- Configure alerting rules
- Test dashboard functionality
- Export dashboard JSON

‚òê **6.6.5** Configure backup and restore
- Document backup procedures
- Create backup scripts
- Document restore procedures
- Test backup and restore
- Document RPO and RTO

‚òê **6.6.6** Create disaster recovery plan
- Document DR procedures
- Create DR checklist
- Document failover process
- Test DR scenarios
- Update documentation

‚òê **6.6.7** Perform security hardening
- Review all configurations
- Enable security features
- Restrict permissions
- Configure network policies
- Run security scan

‚òê **6.6.8** Create deployment checklist
- List pre-deployment requirements
- List deployment steps
- List verification steps
- List post-deployment tasks
- Document rollback procedure

#### Success Criteria:
üîç **Verification Steps:**
- Production manifests are ready
- Helm chart installs successfully
- Secrets are properly managed
- Monitoring dashboards work
- Backups can be restored
- DR plan is tested
- Security is hardened

üìä **Expected Outputs:**
- Production YAML files exist
- Helm chart packages successfully
- Dashboards display all metrics
- Backup completes without errors
- Restore recovers all data
- Security scan passes
- Deployment checklist is complete

---

### Task 6.7: Conduct Production Readiness Review

#### Subtasks:

‚òê **6.7.1** Review system architecture
- Verify all components are deployed
- Check high availability setup
- Verify fault tolerance
- Review scalability limits
- Document any limitations

‚òê **6.7.2** Review security posture
- Verify all security features enabled
- Check authentication and authorization
- Review network segmentation
- Verify encryption at rest and in transit
- Check secret management
- Run security assessment

‚òê **6.7.3** Review operational readiness
- Verify monitoring is comprehensive
- Check alerting is configured
- Verify logging is working
- Check backup and restore
- Review incident response procedures

‚òê **6.7.4** Review performance
- Verify latency meets targets
- Check resource utilization
- Verify throughput is adequate
- Check for bottlenecks
- Review scaling strategy

‚òê **6.7.5** Review documentation
- Verify all docs are complete
- Check all procedures are documented
- Verify runbooks exist
- Check troubleshooting guides
- Ensure documentation is accessible

‚òê **6.7.6** Create readiness report
- Document review findings
- List any issues found
- Prioritize remediation items
- Create action plan
- Set go-live criteria

‚òê **6.7.7** Address critical issues
- Fix any critical findings
- Re-test affected areas
- Update documentation
- Verify fixes

‚òê **6.7.8** Obtain sign-off
- Review with stakeholders
- Address feedback
- Get approval for production
- Document approval

#### Success Criteria:
üîç **Verification Steps:**
- All review areas are covered
- Critical issues are resolved
- Documentation is complete
- Stakeholders are satisfied
- Sign-off is obtained

üìä **Expected Outputs:**
- Readiness report is complete
- All critical issues resolved
- Documentation is up to date
- Stakeholder approval documented
- Ready for production deployment

---

### PHASE 6 COMPLETION CHECKLIST

**Before declaring project complete, verify ALL of the following:**

‚úÖ Comprehensive benchmark suite is complete
‚úÖ All configurations have been evaluated
‚úÖ Pareto frontier analysis is done
‚úÖ Ablation studies are complete
‚úÖ All documentation is finalized
‚úÖ README is comprehensive and accurate
‚úÖ Production manifests are ready
‚úÖ Helm chart is tested
‚úÖ Monitoring dashboards are configured
‚úÖ Backup and restore are tested
‚úÖ Disaster recovery plan is documented
‚úÖ Security hardening is complete
‚úÖ Production readiness review is done
‚úÖ All critical issues are resolved
‚úÖ Stakeholder approval is obtained

**Expected State:**
- Complete, production-ready system
- Comprehensive evaluation completed
- All documentation finalized
- Production deployment ready
- Project successfully completed

**Final Validation:**
- Deploy to production environment
- Run health checks on all components
- Execute end-to-end test scenarios
- Verify monitoring and alerting
- Confirm backup procedures
- Validate disaster recovery
- Monitor system for 24-48 hours
- Declare production go-live

---

## üéØ Project Completion Criteria

### Overall Success Metrics

**Technical Completeness:**
‚úÖ All six phases completed successfully
‚úÖ All components deployed and running
‚úÖ End-to-end flows tested and working
‚úÖ Performance meets targets
‚úÖ Security is hardened

**Evaluation Completeness:**
‚úÖ Benchmark suite executed
‚úÖ All configurations evaluated
‚úÖ Pareto analysis complete
‚úÖ Ablation studies documented
‚úÖ Results published

**Documentation Completeness:**
‚úÖ README is comprehensive
‚úÖ API documentation complete
‚úÖ Deployment guides tested
‚úÖ Operator procedures documented
‚úÖ Developer guides available

**Production Readiness:**
‚úÖ High availability configured
‚úÖ Monitoring operational
‚úÖ Backups working
‚úÖ DR plan tested
‚úÖ Security hardened

**Research Outputs:**
‚úÖ Novel contributions documented
‚úÖ Evaluation methodology defined
‚úÖ Results reproducible
‚úÖ Ready for publication
‚úÖ Open-source release prepared

---

## üìä Progress Tracking Template

Use this template to track your progress through all phases:

```
Phase 0: Foundation & Environment Setup
  Task 0.1: Windows Environment Preparation     [ ] Complete
  Task 0.2: Install Core Development Tools      [ ] Complete
  Task 0.3: Project Repository Setup            [ ] Complete
  Task 0.4: Development Environment Config      [ ] Complete
  Task 0.5: Container Registry Setup            [ ] Complete
  Phase 0 Success Verification                  [ ] Complete

Phase 1: Local Kubernetes Cluster & Infrastructure
  Task 1.1: Create Local Kubernetes Cluster     [ ] Complete
  Task 1.2: Create Kubernetes Namespaces        [ ] Complete
  Task 1.3: Deploy Apache Kafka                 [ ] Complete
  Task 1.4: Deploy TimescaleDB                  [ ] Complete
  Task 1.5: Deploy Redis                        [ ] Complete
  Task 1.6: Deploy Temporal                     [ ] Complete
  Task 1.7: Deploy Prometheus & Grafana         [ ] Complete
  Task 1.8: Configure Secrets Management        [ ] Complete
  Phase 1 Success Verification                  [ ] Complete

Phase 2: eBPF Observability Layer
  Task 2.1: Set Up eBPF Development Env         [ ] Complete
  Task 2.2: Develop File Monitoring eBPF        [ ] Complete
  Task 2.3: Develop Network Monitoring eBPF     [ ] Complete
  Task 2.4: Develop Process Monitoring eBPF     [ ] Complete
  Task 2.5: Build Go Userspace Collector        [ ] Complete
  Task 2.6: Deploy eBPF Collector DaemonSet     [ ] Complete
  Task 2.7: Implement Event Consumer            [ ] Complete
  Phase 2 Success Verification                  [ ] Complete

Phase 3: Security Brain (Policy and ML)
  Task 3.1: Set Up OPA Policy Engine            [ ] Complete
  Task 3.2: Develop Filesystem Policies         [ ] Complete
  Task 3.3: Develop Network Policies            [ ] Complete
  Task 3.4: Develop Tool Governance Policies    [ ] Complete
  Task 3.5: Set Up ML Training Environment      [ ] Complete
  Task 3.6: Train LSTM Anomaly Detection        [ ] Complete
  Task 3.7: Build ML Model Inference Service    [ ] Complete
  Task 3.8: Implement LLM Observer (Claude)     [ ] Complete
  Task 3.9: Build Security Brain Engine         [ ] Complete
  Phase 3 Success Verification                  [ ] Complete

Phase 4: Human-in-the-Loop Workflows
  Task 4.1: Build Tool Proxy Service            [ ] Complete
  Task 4.2: Implement Temporal Workflows        [ ] Complete
  Task 4.3: Set Up Redis Approval Queue         [ ] Complete
  Task 4.4: Build Web UI Dashboard              [ ] Complete
  Task 4.5: Implement Notification Channels     [ ] Complete
  Task 4.6: End-to-End HITL Testing             [ ] Complete
  Phase 4 Success Verification                  [ ] Complete

Phase 5: Red Team Engine and Hardening
  Task 5.1: Set Up Red Team Environment         [ ] Complete
  Task 5.2: Build Attack Generation Agent       [ ] Complete
  Task 5.3: Build Attack Execution Framework    [ ] Complete
  Task 5.4: Build Results Analysis System       [ ] Complete
  Task 5.5: Implement Auto Policy Generation    [ ] Complete
  Task 5.6: Implement Closed-Loop Hardening     [ ] Complete
  Task 5.7: Build Campaign Orchestrator         [ ] Complete
  Phase 5 Success Verification                  [ ] Complete

Phase 6: Evaluation, Documentation & Production
  Task 6.1: Build Comprehensive Benchmark       [ ] Complete
  Task 6.2: Run Configuration Matrix Eval       [ ] Complete
  Task 6.3: Generate Pareto Frontier Analysis   [ ] Complete
  Task 6.4: Run Ablation Studies                [ ] Complete
  Task 6.5: Complete Technical Documentation    [ ] Complete
  Task 6.6: Prepare Production Deployment       [ ] Complete
  Task 6.7: Conduct Production Readiness Review [ ] Complete
  Phase 6 Success Verification                  [ ] Complete

PROJECT COMPLETE                                [ ] ‚úÖ
```

---

## üöÄ Congratulations!

When all checkboxes are complete, you will have successfully built AgentGuard - a production-ready, research-grade, zero-trust security platform for autonomous AI agents!

**What You've Built:**
- Kernel-level observability with eBPF
- Multi-layer security decision engine
- Human oversight workflows
- Automated adversarial testing
- Closed-loop policy hardening
- Comprehensive evaluation framework
- Production-ready deployment

**What You Can Do Next:**
- Deploy to production environments
- Publish research papers
- Present at conferences
- Contribute to open-source community
- Extend with additional features
- Scale to enterprise deployments

**You've achieved something remarkable. Well done!** üéâ